{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "- 1.1. Probability Spaces\n",
    "  - 1.1.1. Definitions\n",
    "  - 1.1.2. Countability\n",
    "  - 1.1.3. The Lebesgue measure\n",
    "  - 1.1.4. Coin tossing\n",
    "  - 1.1.5. Standard and discrete probability spaces\n",
    "  - 1.1.6. Product measures\n",
    "  - 1.1.7. Construction of the Lebesgue measure\n",
    "- 1.2. Random Variables\n",
    "  - 1.2.1. Introduction\n",
    "  - 1.2.2. Random variables\n",
    "  - 1.2.3. Distributions\n",
    "  - 1.2.4. Simples functions\n",
    "  - 1.2.5. Construction of the expectation\n",
    "  - 1.2.6. $L_p$ spaces\n",
    "  - 1.2.7. Computation of the expectation\n",
    "  - 1.2.8. Borel measurability\n",
    "  - 1.2.9. Lebesgue&ndash;Stieltjes integration\n",
    "  - 1.2.10. Probability density functions\n",
    "  - 1.2.11. Statistics of a probability distribution\n",
    "  - 1.2.12. Covariance\n",
    "  - 1.2.13. Random variables on product spaces\n",
    "- 1.3. Probability Distributions\n",
    "  - 1.3.1. Binomial and Bernoulli distributions\n",
    "  - 1.3.2. Poisson approximation of the binomial distribution\n",
    "  - 1.3.3. Poisson distribution\n",
    "  - 1.3.4. Normal approximation to the binomial distribution\n",
    "  - 1.3.5. Gaussian integrals.\n",
    "  - 1.3.6. Normal distribution\n",
    "- 1.4. Conditioning\n",
    "  - 1.4.1. Conditional probability\n",
    "  - 1.4.2. Independence\n",
    "  - 1.4.3. Conditional expectation\n",
    "  - 1.4.4. Conditional probabilities\n",
    "  - 1.4.5. Conditional distributions\n",
    "- 1.5. Multivariate Random Variables\n",
    "  - 1.5.1. Random vectors\n",
    "  - 1.5.2. Variance-covariance matrix\n",
    "  - 1.5.3. Joint probability distributions\n",
    "  - 1.5.4. Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Probability Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1. Definitions.** Probability theory begins with three notions: \n",
    "\n",
    "1. a **sample space** that determines the scope of an experiment,  \n",
    "2. an **event**, which is an element of the same space, that represents a particular outcome of the experiment, and  \n",
    "3. a **probability measure** that assigns the likelihood of to the event.\n",
    "\n",
    "Formally, a **probability space** is an ordered triple $(\\Omega, \\mathcal{F}, \\mathbb{P})$ consisting of a set $\\Omega$ denoting the sample space, a **$\\sigma$-algebra** $\\mathcal{F}$ of events, and a **probability measure** $\\mathbb{P}$ on the measurable space $(\\Omega, \\mathcal{F})$.\n",
    "\n",
    "By a $\\sigma$-algebra, we mean a set $\\mathcal{F}$ of subsets of $\\mathcal{F}$ that satisfies the following properties:\n",
    "\n",
    "1. The full set $\\mathcal{F}$ and the empty set $\\varnothing$ are elements of $\\mathcal{F}$;\n",
    "2. if $E$ is an element of $\\mathcal{F}$, then its complement $X \\smallsetminus E$ is an element of $\\mathcal{F}$;\n",
    "3. if $\\{E_n\\}_{n=1}^{\\infty}$ is a collection of sets in $\\mathcal{F}$, then its union $\\bigcup_{n=1}^\\infty E_n$ and its intersection $\\bigcap_{n=1}^\\infty E_n$ are elements of $\\mathcal{F}$.  \n",
    "\n",
    "The axioms above formalize the basic, intuitive properties of a sample space. Indeed, we would want to be able to examine, for each event $E$, the complement event $X \\smallsetminus E$ of $E$ not occurring. We would also want to be able to examine the event that at least one of the events in a collection of events happens, and the event that all events in a collection happens. Lastly, we throw in the two trivial events for completeness: the event that something happens, and the event that nothing happens.\n",
    "\n",
    "The ordered pair $(\\Omega, \\mathcal{F})$ of a set and a $\\sigma$-algebra on it is called a **measurable space**, because we can define a probability measure on it. A **probability measure** on $(\\Omega, \\mathcal{F})$ is a function $\\mathbb{P}:\\Omega \\to [0,1]$ such that $\\mathbb{P}(\\Omega) = 1$, and that the **countable additivity** criterion\n",
    "\n",
    "$$\\mathbb{P}\\left(\\bigcup_{n=1}^\\infty E_n \\right) = \\sum_{n=1}^\\infty \\mathbb{P}(E_n)$$\n",
    "\n",
    "holds whenever $\\{E_n\\}_{n=1}^\\infty$ is a disjoint collection of events. In other words, the probably of at least one of the events in a disjoint collection of events happens should be the sum of the individual probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2. Countability.** Observe that we consider only *countable* unions (and intersections) in formalizing probability theory. This is to prevent pathological behaviors, such as events of probability zero adding up to be an event of probability one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3. The Lebesgue measure.** The standard example of a probability space is $\\mathcal{S}  = ([0,1],\\mathscr{B}_{[0,1]},\\mathscr{L}_{[0,1]})$, the Lebesgue measure on the Borel $\\sigma$-algebra on the unit interval $[0,1]$. The Lebesgue measure is the standard *length* of sets of real numbers. For example, the Lebesgue measure of $(0,1/3) \\cup (2/3,1)$ is $(1/3-0) + (1-2/3) = 2/3$. The Borel $\\sigma$-algebra collects the sets of real numbers that are *nice enough* to have a reasonable notion of length. (Not so nice sets exist: see [Vitali set](https://en.wikipedia.org/wiki/Vitali_set).) A formal construction of the Lebesgue measure (**§1.1.7**) is done via the Carathéodory extension theorem (**§1.1.6**).\n",
    "\n",
    "Since <a href=\"https://en.wikipedia.org/wiki/Singleton_(mathematics)\">singleton sets</a> have Lebesgue measure zero, we typically consider two events whose intersection consists of finitely many points to be disjoint. This is an example of a property that holds **almost surely**, i.e., a statement that is true except on a set of measure zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4. Coin tossing.** Many probabilistic contexts can be modeled on $\\mathcal{S}$. For example, consider the experiment of tossing an unbiased coin twice, resulting in four outcomes, each with probability $1/4$: `HH`, `HT`, `TH`, `TT`. We can let $[0,1/4]$, $[1/4, 1/2]$, $[1/2, 3/4]$, $[3/4,1]$ represent the four outcomes to model this experiment. \n",
    "\n",
    "With this basic setting, compound events can be represented by subsets of $[0,1]$ as well. For example, the event that the first coin toss turns up head is $[0, 1/4] \\cup [1/4,1/2]$. All things considered, the $\\sigma$-algebra for our coin-toss experiment is\n",
    "\n",
    "$$\\begin{align*}\\mathcal{F} =& \\{[p_1,q_1] \\cup [p_2, q_2] : p_1,p_2,q_1,q_2 \\in \\{0,1/4,1/2,/3/4,1\\} \\\\ &\\mbox{ and } p_1 < q_1 \\leq p_2 < q_2\\} \\cup \\{\\varnothing\\},\\end{align*}$$\n",
    "\n",
    "and the probability measure is defined by the Lebesgue measure on each set in $\\mathcal{F}$.\n",
    "\n",
    "Analogously, the experiment of tossing an unbiased coin $n$ times can be modeled with the $\\sigma$-algebra\n",
    "\n",
    "$$\\mathcal{F}_n =\\sigma \\left(\\left\\{\\left[ \\frac{k}{2^n}, \\frac{k+1}{2^n}\\right] : k \\in \\{0,1,\\ldots,2^n-1\\}\\right\\}\\right)$$\n",
    "\n",
    "and the Lebesgue measure as the probability measure. Here, $\\mathcal{F}(\\mathcal{A})$ denotes the $\\sigma$-algebra constructed from a collection $\\mathcal{A}$ by taking countable unions, countable intersections, and set complements. Such a $\\sigma$-algebra is called the **$\\sigma$-algebra generated by $\\mathcal{A}$.**\n",
    "\n",
    "How about tossing an unbiased coin *infinitely* many times? It would be reasonable to take the $\\sigma$-algebra to be\n",
    "\n",
    "$$\\mathcal{F}_\\infty = \\sigma\\left( \\bigcup_{n=1}^\\infty \\mathcal{F}_n \\right),$$\n",
    "\n",
    "with the Lebesgue measure as the probability measure. Since every real number can be written as an infinite sum of [dyadic rational numbers](https://en.wikipedia.org/wiki/Dyadic_rational), $\\mathcal{F}_\\infty$ contains all subintervals of $[0,1]$. Now, the Borel $\\sigma$-algebra $\\mathscr{B}_{[0,1]}$ can be generated by taking countable unions and intersections of the subintervals of $[0,1]$, we conclude that $\\mathcal{F}_\\infty \\supseteq \\mathscr{B}_{[0,1]}$. But then $\\mathcal{F}_\\infty$ consists of subsets of $\\mathscr{B}_{[0,1]}$, and so $\\mathcal{F}_\\infty = \\mathscr{B}_{[0,1]}$. It follows that the probability space $([0,1], \\mathscr{B}_{[0,1]}, \\mathscr{L}_{[0,1]})$ models the experiment of tossing an unbiased coin infinitely many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.5. Standard and discrete probability spaces.** Often, experiments that appear to have nothing to do with the unit interval at first glance turn out to admit $([0,1], \\mathscr{B}_{[0,1]}, \\mathscr{L}_{[0,1]})$ as its probabilistic model. A probability space modeling an experiment that can also be modeled on $([0,1], \\mathscr{B}_{[0,1]}, \\mathscr{L}_{[0,1]})$ is called a [standard probability space](https://en.wikipedia.org/wiki/Standard_probability_space).\n",
    "\n",
    "Although standard probability spaces can be modeled on $([0,1], \\mathscr{B}_{[0,1]}, \\mathscr{L}_{[0,1]})$, there is often a more abstract probabilistic model that is more in tune with the nature of the experiment. For example, we can model the experiment of tossing an unbiased coin $$n$$ times quite directly, using symbols `H` and `T`. Indeed, we take the $n$-fold Cartesian product\n",
    "\n",
    "$$\\Omega_n = \\{H,T\\}^n = \\{(t_1,\\ldots,t_n) : t_1,\\ldots,t_n \\in \\{H,T\\}\\}$$\n",
    "\n",
    "to be the sample space, the [power set](https://en.wikipedia.org/wiki/Power_set) \n",
    "\n",
    "$$\\mathcal{F}_n = \\mathcal{P}(\\Omega_n) = \\{A : A \\subseteq \\Omega_n\\}$$\n",
    "\n",
    "of the sample space to be the $\\sigma$-algebra of events, and the normalized [counting measure](https://en.wikipedia.org/wiki/Counting_measure)\n",
    "\n",
    "$$\\mathbb{P}[E] = \\frac{|E|}{2^n} \\mbox{ for each } A \\in \\mathcal{F}_n$$\n",
    "\n",
    "to be the probability measure. $(\\Omega_n, \\mathcal{F}_n,\\mathbb{P})$ is an example of a **discrete probability space**, whose sample space $\\Omega_n$ consists of at most countably many elements and whose $\\sigma$-algebra is the power set of the sample space.\n",
    "\n",
    "On the other hand, it is not easy to see what the natural probability measure for the infinite coin toss space $(\\{H,T\\}^\\infty, \\mathcal{P}(\\{H,T\\}^n))$ should be. In these cases, it makes sense to model the experiment on the unit interval and use the Lebesgue measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.6. Product measures** To formalize the process of taking the product of probability spaces, we consider the Cartesian product $\\Omega_1 \\times \\Omega_2$ of the samples spaces $\\Omega_1$ and $\\Omega_2$ underlying the probability spaces $(\\Omega_1,\\mathcal{F}_1,\\mathbb{P}_1)$ and $(\\Omega_2,\\mathcal{F}_2,\\mathbb{P}_2)$, respectively. A **rectangle** on $\\Omega_1 \\times \\Omega_2$ is a set of the form\n",
    "\n",
    "$$E_1 \\times E_2,$$\n",
    "\n",
    "where $E_1 \\in \\mathcal{F}_1$ and $E_2 \\in \\mathcal{F}_2$. We define the **product probability measure** $\\mathbb{P}_1 \\otimes \\mathbb{P}_2$ on rectangles by setting\n",
    "\n",
    "$$(\\mathbb{P}_1 \\otimes \\mathbb{P}_2)(E_1 \\times E_2) = \\mathbb{P}_1(E_1)\\mathbb{P}_2(E_2).$$\n",
    "\n",
    "Since the collection of all rectangles is not a $\\sigma$-algebra, $\\mathbb{P}_1 \\otimes \\mathbb{P}_2$ is not yet a *bona fide* probability measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider a collection $\\mathcal{F}$ of all finite unions, intersections, and complements of rectangles on $\\Omega_1 \\times \\Omega_2$. The resulting $\\mathcal{F}$ is an **algebra**, as it is closed under finite unions, intersections, and complementation. The following extension theorem now extends $\\mathbb{P}_1 \\otimes \\mathbb{P}_2$ to $\\sigma(\\mathcal{F})$, the $\\sigma$-algebra generated by $\\mathcal{F}$ (**§1.1.4**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Carathéodory extension theorem.** If $\\mathbb{P}$ is a countably additive (**§1.1.1**) function on an algebra $(\\Omega, \\mathcal{A})$ with $\\mathbb{P}(\\Omega) = 1$, then there exists a unique probability measure on $(\\Omega, \\sigma(\\mathcal{A}))$ that agrees with $\\mathbb{P}$ on $\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $\\sigma$-algebra is called the **product $\\sigma$-algebra** of $\\mathcal{F}_1$ and $\\mathcal{F}_2$ and is denoted by $\\mathcal{F}_1 \\otimes \\mathcal{F}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.7. Construction of the Lebesgue measure.** With the Carathéodory extension theorem, we can give a more detailed construction of the Lebesgue measure (**§1.1.3**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a metric space $(M,d_M)$, the $\\sigma$-algebra $\\mathscr{B}_M$ generated by the open subsets of $M$ is called the **Borel $\\sigma$-algebra** on $M$. Equivalently, $\\mathscr{B}_M$ is the $\\sigma$-algebra generated by the closed subsets of $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the **length** $\\mathscr{L}$ of an open subinterval $(a,b)$ of $[0,1]$ to be $b-a$. Applying the Carathéodory extension theorem to $\\mathscr{L}$ on the field of all finite unions, intersections, and complements of open subintervals of $[0,1]$, we obtain a probability measure on $\\mathscr{B}_{[0,1]}$, called the **Lebesgue measure** on $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a **measure** is a countably additive function on a $\\sigma$-algebra. Carathéodory extension theorem continues to hold for general measures, producing not a probability measure but merely a measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the length of an open interval $(a,b)$ in $\\mathbb{R}$ to be $b-a$, we can use the Carathéodory extension theorem to construct a measure on $\\mathscr{B}_\\mathbb{R}$, called the **Lebesgue measure on $\\mathbb{R}$**. The product $\\sigma$-algebra\n",
    "\n",
    "$$\\mathscr{B}_\\mathbb{R} \\otimes \\cdots \\otimes \\mathscr{B}_{\\mathbb{R}}$$\n",
    "\n",
    "agrees with the Borel $\\sigma$-algebra $\\mathscr{B}_{\\mathbb{R}^n}$ with respect to the standard Euclidean metric, and the resulting product measure is called the **Lebesgue measure on $\\mathbb{R}^n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.1. Introduction.** To attach meanings to the outcomes of an experiments, it makes sense to consider functions on the corresponding sample space, the distribution of the values of such a function over the sample space, and the weighted average of the values that take the distribution into account. These correspond to random variables, probability distributions, and expectations, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2. Random variables.** For various technical reasons, we do not consider every function on a sample space. Instead, we consider **measurable functions** on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, which are functions $X: \\Omega \\to \\mathbb{R}$  such that\n",
    "\n",
    "$$X^{-1}((-\\infty,\\alpha]) = \\{X \\leq \\alpha\\} = \\{\\omega : X(\\omega) \\leq \\alpha\\}$$\n",
    "\n",
    "is an element of the $\\sigma$-algebra $\\mathcal{F}$ for each real number $\\alpha$. A measurable function defined over a probability space is called a **random variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3. Distributions.** One reason for this restriction is that we are interested in the **probability distribution**, or the **cumulative distribution function**, \n",
    "\n",
    "$$F_X(\\alpha) = \\mathbb{P}[X \\leq \\alpha] = \\mathbb{P}[\\{x: X(\\omega) \\leq \\alpha\\}]$$\n",
    "\n",
    "of a random variable $X$, which tells us how the values of $X$ are distributed throughout the sample space. The expression $\\mathbb{P}[X \\leq \\alpha]$ only makes sense if $\\{X \\leq \\alpha\\}$ is in $\\mathcal{F}$, i.e., $X$ is measurable.\n",
    "\n",
    "Now that there is a canonical interpretation of $F_X$ as a probability measure on $\\mathbb{R}$. We set\n",
    "\n",
    "$$\\mathscr{L}_X((-\\infty, \\alpha]) = F_X(\\alpha)$$\n",
    "\n",
    "for each $\\alpha \\in \\mathbb{R}$ and apply the Carathéodory extension theorem (**§1.1.6**) to extend $\\mathscr{L}_X$ to a probability measure on $(\\mathbb{R},\\mathscr{B}_\\mathbb{R})$. This measure is called the **law** with respect to the random variable $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that $F_X$ satisfies the following properties:\n",
    "\n",
    "- $0 \\leq F_X \\leq 1$;\n",
    "- $F_X$ is increasing;\n",
    "- $F_X(\\alpha) \\to 0$ as $\\alpha \\to -\\infty$;\n",
    "- $F_X(\\alpha) \\to 1$ as $\\alpha \\to \\infty$;\n",
    "- $F_X$ is [right-continuous](https://en.wikipedia.org/wiki/Continuous_function#Directional_and_semi-continuity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, any function $F$ that satisfies the above properties gives rise to a random variable $X$ such that $F = F_X$ (**§1.2.9**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.4. Simple functions.** Another reason for the measurability restriction is that measurable functions, however complicated they might be, can be approximated by a class of functions known as **simple functions**. A simple function on $(\\Omega, \\mathcal{F}, \\mathbb{P})$ is a [linear combination](https://en.wikipedia.org/wiki/Linear_combination) of [indicator functions](https://en.wikipedia.org/wiki/Indicator_function), i.e.,\n",
    "\n",
    "$$s(\\omega) = \\sum_{i=1}^k a_i \\boldsymbol{1}_{E_i}(\\omega,$$\n",
    "\n",
    "where $\\boldsymbol{1}_{E_i}(\\omega)$ is 1 if $\\omega \\in E_i$ and 0 elsewhere. We assume that $E_1,\\ldots,E_k$ are disjoint events, i.e., elements of the $\\sigma$-algebra $\\mathcal{F}$. \n",
    "\n",
    "It is not hard to work out the probability distribution and the weighted average of a simple function. Indeed, given a simple function $s(\\omega) = \\sum_{i=1}^k a_i \\boldsymbol{1}_{E_i}(\\omega)$, we see that the probability distribution of $s$ is\n",
    "\n",
    "$$F_s(\\alpha) = \\mathbb{P}[s \\leq \\alpha] = \\sum_{\\substack{1 \\leq i \\leq k \\\\\\ a_i \\leq \\alpha}} \\mathbb{P}(E_i).$$\n",
    "\n",
    "The weight average, or the **expectation**, of $s$ would simply be the weighted average of the probability measures on the events $E_1,\\ldots,E_k$, i.e.,\n",
    "\n",
    "$$\\mathbb{E}[s] = \\sum_{i=1}^k a_i \\mathbb{P}[E_k].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.5. Construction of the expectation.** Whenever a random variable $X$ on $\\Omega$ is nonnegative, there exists a sequence $(s_n)_{n=1}^\\infty$ of simple functions such that $0 \\leq s_1 \\leq s_2 \\leq \\ldots \\leq X$ and that\n",
    "\n",
    "$$\\lim_{n \\to \\infty} s_n(\\omega) = X(\\omega)$$\n",
    "\n",
    "for all $\\omega \\in X$. With this approximation, we can generalize the notion of expectation to a larger class of random variables. Indeed, we define\n",
    "\n",
    "$$\\mathbb{E}[X] = \\lim_{n \\to \\infty} \\mathbb{E}[s_n],$$\n",
    "\n",
    "for any nonnegative random variable $X$. While $X$ could have more than one simple function approximation, the [machinery of measure theory](https://terrytao.wordpress.com/2010/09/25/245a-notes-3-integration-on-abstract-measure-spaces-and-the-convergence-theorems/) guarantees that the limit is the same in each case.\n",
    "\n",
    "Now, every real-valued random variable $X$ can be written as the difference $X^+ - X^-$, where $X^+ = \\max(X,0)$ and $X^- = \\max(-X,0)$. We can then define the **expectation,** or the **mean**, of $X$ to be the sum\n",
    "\n",
    "$$\\mathbb{E}[X] = \\mathbb{E}[X^+] - \\mathbb{E}[X^-].$$\n",
    "\n",
    "By construction, $\\mathbb{E}$ is linear. In other words, if $X$ and $Y$ are random variables, then\n",
    "\n",
    "$$\\mathbb{E}[aX+bY] = a\\mathbb{E}[X]+ b\\mathbb{E}[Y].$$\n",
    "\n",
    "We remark that we often drop the brackets and write $\\mathbb{E}X$ to denote the expectation of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exepctation constructed this way satisfies the limit theorem\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\mathbb{E}[X_n] = \\mathbb{E}\\left[ \\lim_{n \\to \\infty} X_n \\right]$$\n",
    "\n",
    "whenever $X_1 \\leq X_2 \\leq \\cdots \\leq X_n \\leq \\cdots $ or there exists a random variable $Y$ such that $\\mathbb{E}[\\vert Y \\vert] < \\infty$ and $\\vert X_n \\vert \\leq Y$ for all $n$. The former is called the **monotone convergence theorem**; the latter is called the **dominated convergence theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.6. $L_p$ space.** For each $1 \\leq p < \\infty$, we define $L_p(\\Omega,\\mathcal{F},\\mathbb{P})$ to be the collection of all random variables $X$ such that\n",
    "\n",
    "$$\\|X\\|_p = \\mathbb{E}[\\vert X \\vert^p]^{1/p} < \\infty.$$\n",
    "\n",
    "Identifying any pair of random variables $X$ and $Y$ satisfying $\\|X - Y\\|_p = 0$, we see that the vector space $L_p(\\Omega,\\mathcal{F},\\mathbb{P})$ is a Banach space under the $L_p$-norm $\\|\\cdot\\|_p$. If $p =2 $, then $L_p(\\Omega,\\mathcal{F},\\mathbb{P})$ is a Hilbert space with inner product\n",
    "\n",
    "$$\\langle X, Y \\rangle_{L_2} = \\mathbb{E}[ \\vert XY \\vert ].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the **essential supremum** of a random variable $X$ to be the quantity\n",
    "\n",
    "$$\\inf \\{t \\in \\mathbb{R} : \\mathbb{P}[X > t] = 0\\}$$\n",
    "\n",
    "and denote it by $\\|X\\|_\\infty$. The collection of all random variables with finite essential supremum is denoted by $L_\\infty(\\Omega,\\mathcal{F},\\mathbb{P})$. Identifying any pair of random variables $X$ and $Y$ satisfying $\\|X - Y\\|_\\infty = 0$, we see that $L_\\infty(\\Omega,\\mathcal{F},\\mathbb{P})$ is a Banach space under the $L_\\infty$-norm $\\|\\cdot\\|_\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L_p$ norms are related by **Hölder's inequality**\n",
    "\n",
    "$$\\|XY\\|_1 \\leq \\|X\\|_p \\|Y\\|_{p'}.$$\n",
    "\n",
    "where $\\frac{1}{p} + \\frac{1}{p'} = 1$. Here we consider $1/\\infty=0$. The $p=2$ case can be written as\n",
    "\n",
    "$$\\left\\vert \\langle X, Y \\rangle_{L_2} \\right\\vert  \\leq \\|X\\|_2 \\|Y\\|_2,$$\n",
    "\n",
    "known as the **Cauchy&ndash;Schwarz inequality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever $p \\leq q$, we see that\n",
    "\n",
    "$$\\| X^{q/p} \\|_1^{p/q} \\leq \\|X^{q/p}\\|_p^{p/q} \\|1\\|_{p'}^{p/q} = \\|X\\|_q,$$\n",
    "\n",
    "where $\\frac{1}{p} + \\frac{1}{p'} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix $p \\leq q$ and suppose that $X \\in \\mathcal{L}_q(\\Omega,\\mathcal{F},\\mathbb{P})$. Observe that\n",
    "\n",
    "$$\\|X\\|_p = \\|\\vert X \\vert^p\\|_1^{1/p} \\leq \\|\\vert X \\vert^p\\|_{q/p}^{1/p}\\|1\\|_{(q/p)'}^{1/p},$$\n",
    "\n",
    "where $1/(p/q) + 1/(p/q)' = 1$. Hölder's inequality holds, as $q/p \\geq 1$. Since the total measure of $\\Omega$ is 1, we see that $\\|1\\|_{(q/p)'} = 1$, and so\n",
    "\n",
    "$$\\|X\\|_p \\leq \\|\\vert X \\vert^p\\|_{q/p}^1/p = \\left(\\mathbb{E}\\left[\\left(\\vert X \\vert ^p\\right)^{q/p}\\right]^{1/p}\\right)^{p/q} = \\mathbb{E}[ \\vert X \\vert^q ]^{1/q} = \\|X\\|_q.$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\\|X\\|_p \\leq \\|X\\|_q.$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\\|X\\|_p = \\mathbb{E}[\\vert X \\vert^p]^{1/p} \\leq \\mathbb{E}[ \\|X\\|_\\infty^p ] ^{1/p} = \\|X\\|_\\infty \\mathbb{E}[1] = \\|X\\|_\\infty$$\n",
    "\n",
    "for all $p \\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.7. Computation of the expectation** On $([0,1],\\mathscr{B}_{[0,1]},\\mathscr{L}_{[0,1]})$, this construction of the expectation agrees with the standard integral on $[0,1]$, a fact we often use to compute the expectations of random variables.\n",
    "\n",
    "On a discrete probability space, the **probability mass function**\n",
    "\n",
    "$$p_X(a) = \\mathbb{P}[X = a]$$\n",
    "\n",
    "is also of interest. Note that, in this case, there are at most countably many values in the <a href=\"https://en.wikipedia.org/wiki/Range_(mathematics)\">range</a> $\\operatorname{im} X$ of $X$, and so we can write the expectation of $X$ as a sum:\n",
    "\n",
    "$$\\mathbb{E}X = \\sum_{a \\in \\operatorname{im} X} a p_X(a).$$\n",
    "\n",
    "Observe, in this case, that \n",
    "$\\mathbb{E}X^k = \\sum_{a \\in \\operatorname{im} X} a^k p_X(a)$\n",
    "whenever $k \\in \\mathbb{N}$. To see this, we note that $\\{X = a\\}$ and $\\{X = b\\}$ are disjoint whenever $a = b$. Therefore,\n",
    "\n",
    "$$X^k(x) = \\left(\\sum_{a \\in \\operatorname{im} X} a \\boldsymbol{1}_{\\{X = a\\}} \\right)^k = \\left(x \\boldsymbol{1}_{\\{X = x\\}} \\right)^k = x^k \\boldsymbol{1}_{\\{X = x\\}}$$\n",
    "\n",
    "for each $x$.\n",
    "\n",
    "Another useful tool for computing expectations is the **change-of-variables formula**\n",
    "\n",
    "$$\\mathbb{E}g(X) = \\int_{-\\infty}^\\infty g(\\alpha) \\, dF_X(\\alpha),$$  \n",
    "\n",
    "which holds whenever $g:\\mathbb{R} \\to \\mathbb{R}$ is a function that makes $g(X)$ measurable and satisfies the decay condition $\\mathbb{E} \\vert g(X) \\vert < \\infty$. The integral is to be understood as a **Lebesgue-Stieltjes integral**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.8. Borel measurability.** What functions $g$ would preserve the measurability of a random variable $X$ upon composition? Recall that $X$ is a (measurable) random variable on $(\\Omega,\\mathcal{F},\\mathbb{P})$ if $X^{-1}((-\\infty,\\alpha])$ is in $\\mathcal{F}$ for each real number $\\alpha$. Since \n",
    "\n",
    "$$\\begin{align*} X^{-1} \\left(\\bigcup_{n=1}^\\infty E_n\\right) &= \\bigcup_{n=1}^\\infty X^{-1}(E_n) \\\\ X^{-1} \\left( \\bigcap_{n=1}^\\infty E_n \\right) &= \\bigcap_{n=1}^\\infty X^{-1}(E_n) \\\\ X^{-1}(\\mathbb{R} \\smallsetminus E_1) &= \\Omega \\smallsetminus X^{-1}(E_1)\\end{align*}$$\n",
    "\n",
    "for any collection of subsets $\\{E_n\\}_{n=1}^\\infty$ of $\\mathbb{R}$, it follows that $X$ is measurable if and only if $X^{-1}(E) \\in \\mathcal{F}$ for all $E \\in \\sigma(\\mathcal{I})$, the $\\sigma$-algebra generated by the collection $\\mathcal{I}$ of all intervals of the form $(-\\infty, \\alpha]$. Since $(-\\infty, a] \\cap (-\\infty, b] = [a,b]$ whenever $a \\leq b$, the $\\sigma$-algebra $\\sigma(\\mathcal{I})$ contains all closed intervals. Now,\n",
    "\n",
    "$$(a,b) = \\bigcup_{n=1}^\\infty \\left[a - \\frac{\\vert b -a \\vert}{3^n} , b - \\frac{\\vert b - a\\vert}{3^n} \\right],$$\n",
    "\n",
    "and so $\\sigma(\\mathcal{I})$ contains all open intervals. It follows that $\\sigma(\\mathcal{I}) = \\mathscr{B}_\\mathbb{R}$, the Borel $\\sigma$-algebra on $\\mathbb{R}$ (**§1.1.7**).\n",
    "\n",
    "We now observe that, given a random variable $X$, the composite function $g(X)$ remains measurable if $g$ preserves the Borel-ness of sets. In other words, if $g^{-1}(E) \\in \\mathscr{B}_{\\mathbb{R}}$ whenever $E \\in \\mathscr{B}_{\\mathbb{R}}$, then we have $(g(X))^{-1}(E) \\in \\Omega$ whenever $E \\in \\mathscr{B}_{\\mathbb{R}}$, thereby preserving the measurability of $X$. Such a function $g:\\mathbb{R} \\to \\mathbb{R}$ is called **Borel measurable on $\\mathbb{R}$**. We remark that all [continuous functions](https://en.wikipedia.org/wiki/Continuous_function) are Borel measurable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.9. Lebesgue&ndash;Stieltjes integration** We recall that the distribution function\n",
    "\n",
    "$$F_X(\\alpha) = \\mathbb{P}[X \\leq \\alpha]$$\n",
    "\n",
    "of a random variable $X$ satisfies the following properties:\n",
    "\n",
    "- $0 \\leq F_X \\leq 1$;\n",
    "- $F_X$ is increasing;\n",
    "- $F_X(\\alpha) \\to 0$ as $\\alpha \\to -\\infty$;\n",
    "- $F_X(\\alpha) \\to 1$ as $\\alpha \\to \\infty$;\n",
    "- $F_X$ is [right-continuous](https://en.wikipedia.org/wiki/Continuous_function#Directional_and_semi-continuity).\n",
    "\n",
    "We now suppose that $F:\\mathbb{R} \\to [0,1]$ satisfies the above properties. The set function $dF$ given by the formula\n",
    "\n",
    "$$dF((-\\infty,\\alpha]) = F(\\alpha)$$\n",
    "\n",
    "can be extended (**§1.1.6**)  to a probability measure on $(\\mathbb{R},\\mathscr{B}_\\mathbb{R})$. The measure $dF$ is called the **Lebesgue&ndash;Stieltjes measure associated with $F$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the function $X:[0,1] \\to \\mathbb{R}$ defined by the formula\n",
    "\n",
    "$$X(\\omega) = \\sup\\{t : F(t) \\leq \\omega\\}$$\n",
    "\n",
    "is a random variable on $([0,1],\\mathscr{B}_{[0,1]},\\mathscr{L}_{[0,1]})$ such that\n",
    "\n",
    "$$F_X = F.$$\n",
    "\n",
    "It follows that there is one-to-one correspondence between probability distributions and the Lebesgue&ndash;Stieltjes measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As an example, we consider the [Heaviside function](https://en.wikipedia.org/wiki/Heaviside_step_function)\n",
    "\n",
    "$$H(x) = \\begin{cases} 0 & \\mbox{ if } x < 0; \\\\ 1 & \\mbox{ if } x > 0.\\end{cases}$$\n",
    "\n",
    "The corresponding probability measure is\n",
    "\n",
    "$$dH = \\sum_{i=1}^n \\mathbb{P}[E_i] \\delta_{a_i},$$\n",
    "\n",
    "where $\\delta_{a_i}$ is the [Dirac delta measure](https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_measure)\n",
    "\n",
    "$$\\delta_{a_i}(E) = \\begin{cases} 1 & \\mbox{ if } a_i \\in E \\\\ 0 & \\mbox{ otherwise.}\\end{cases}$$\n",
    "\n",
    "on $\\mathbb{R}$. Therefore, the expectation of a simple function $s$ on $(\\mathbb{R},\\mathscr{B}_{\\mathbb{R}},dH)$ is\n",
    "\n",
    "$$\\sum_{i=1}^n s(a_i) \\mathbb{P}[E_i],$$\n",
    "\n",
    "whence it follows from the construction of the expectation (**§1.2.5**) that\n",
    "\n",
    "$$\\mathbb{E}g(X) = \\int_{-\\infty}^\\infty g(\\alpha) \\, dH(\\alpha) = \\sum_{i=1}^n g(a_i) \\mathbb{P}[E_i].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.10. Probability density functions.** We now suppose that $F_X$ is differentiable, and that $f_X$ is the derivative of $F_X$. In this case,  a standard result in the theory of Lebesgue-Stieltjes integral is that\n",
    "\n",
    "$$\\mathbb{E}g(X) = \\int_{-\\infty}^\\infty g(\\alpha) dF_X(\\alpha) = \\int_{-\\infty}^\\infty g(\\alpha) f_X(\\alpha) \\, d\\alpha.$$\n",
    "\n",
    "In particular, if $g(x) = x$, then\n",
    "\n",
    "$$\\mathbb{E}X = \\int_{-\\infty}^\\infty \\alpha f_X(\\alpha) \\, d\\alpha.$$\n",
    "\n",
    "$f_X$ is referred to as the **probability density function** of $X$.\n",
    "\n",
    "Probability density functions are useful for computational purposes, as they convert the task of computing the expectation into integration on the real line. Moreover, some probability distributions that do not admit neat characterizations have closed-form expressions for their density functions. For example, the **standard normal distribution** (**§1.3.6**) has probability density funciton\n",
    "\n",
    "$$f_X(t) = \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2},$$\n",
    "\n",
    "but its distribution function\n",
    "\n",
    "$$F_X(\\alpha) = \\int_{-\\infty}^t \\frac{1}{\\sqrt{2\\pi}} e^{-t^2/2} \\, dt$$\n",
    "\n",
    "does not have a closed-form expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.11. Statistics of a probability distribution.** We now turn to a few **statistics** of a probability distribution, i.e., numbers that summarize information about the distribution.\n",
    "\n",
    "We already know about the mean $\\mathbb{E}X$ of $X$. Another useful *middle value* is the **median** $F_X^{-1}(1/2)$, where the **quantile function** $F_X^{-1}$ is given by the formula\n",
    "\n",
    "$$F^{-1}(q) = \\inf \\{x : F_X(x) > q \\}.$$\n",
    "\n",
    "$F_X^{-1}(1/4)$ and $F^{-1}(3/4)$ are called the **first quartile** and the **third quartile**, respectively.\n",
    "\n",
    "Often, we are interested not only in the mean of $X$, but also the **variance**\n",
    "\n",
    "$$\\operatorname{var}[E] = \\mathbb{E}[(X-\\mathbb{E}X)^2]$$\n",
    "\n",
    "of $X$, which measures the *spread* of a distribution. While it might make sense, at a first glance, to use $\\mathbb{E}[X - \\mathbb{E}X]$ to measure the spread, the linearity of $\\mathbb{E}$ implies that\n",
    "\n",
    "$$\\begin{align*} \\mathbb{E}[X-\\mathbb{E}X] &= \\mathbb{E}[X - \\mathbb{E}X\\boldsymbol{1}_{\\Omega}] \\\\\\ &= \\mathbb{E}X - \\mathbb{E}[\\mathbb{E}X\\boldsymbol{1}_{\\Omega}] \\\\\\ &= \\mathbb{E}X - \\mathbb{E}X[\\boldsymbol{1}_{\\Omega}] \\\\\\ &= \\mathbb{E}X - \\mathbb{E}X = 0.\\end{align*}$$ \n",
    "\n",
    "Similar calculations yield $\\operatorname{var}[X] = \\mathbb{E}X^2 - (\\mathbb{E}X)^2$.\n",
    "\n",
    "The **standard deviation** of $X$ is\n",
    "\n",
    "$$\\operatorname{std}[X] = \\sqrt{\\operatorname{var}[E]}$$\n",
    "\n",
    "and is often denoted by $\\sigma$. The standard deviation is useful for modeling purposes because it has the same [unit](https://en.wikipedia.org/wiki/Base_unit_(measurement)) as the random variable $X$.\n",
    "\n",
    "In general, the **$n$th moment** of $X$ is the expectation\n",
    "\n",
    "$$\\mu_n = \\mu_n(X) = \\mathbb{E}[X^n].$$\n",
    "\n",
    "The first moment, the mean, is typically denoted by $\\mu$.\n",
    "\n",
    "A neat way to compute the moments of $X$ is to consider its [moment-generating function](https://en.wikipedia.org/wiki/Moment-generating_function)\n",
    "\n",
    "$$M_X(t) = \\mathbb{E}[e^{tX}].$$\n",
    "\n",
    "Indeed, we observe that\n",
    "\n",
    "$$\\mathbb{E}[e^{tX}] = \\mathbb{E} \\left[ \\sum_{n=0}^\\infty \\frac{(tX)^n}{n!}\\right] = \\sum_{n=0}^\\infty \\frac{t^n \\mu_n}{n!}$$\n",
    "\n",
    "by the [uniform convergence](https://en.wikipedia.org/wiki/Uniform_convergence) of Taylor series. It then follows that \n",
    "\n",
    "$$ \\frac{d^kM_X(t)}{dt^k}= \\sum_{n=k}^\\infty \\frac{t^{n-k} \\mu_n}{(n-k)!},$$\n",
    "\n",
    "whence\n",
    "\n",
    "$$\\frac{d^kM_X(t)}{dt^k}\\bigg|_{t = 0}\\mathbb{E}[e^{tX}] = \\mu_k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.12. Covariance.** Recall that the **variance** of a random variable $X$ is\n",
    "\n",
    "$$\\mathbb{E}[(X-\\mathbb{E}X)^2] = \\mathbb{E}[(X-\\mathbb{E}X)(X -\\mathbb{E}X)].$$\n",
    "\n",
    "The **covariance** between two random variables $X$ and $Y$ is defined analogously:\n",
    "\n",
    "$$\\operatorname{Cov}(X,Y) = \\mathbb{E}[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the covariance is zero when $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$. This is often obtained as a consequence of independence (**§1.5.2**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance and covariance are related in the following manner:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\operatorname{var}[X+Y] &= \\operatorname{var}[X] + \\operatorname{var}[Y] + 2 \\operatorname{Cov}(X,Y); \\\\\n",
    "\\operatorname{var}[X-Y] &= \\operatorname{var}[X] + \\operatorname{var}[Y] - 2 \\operatorname{Cov}(X,Y).\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the covariance, we obtain the **correlation coefficient**\n",
    "\n",
    "$$\\rho = \\rho_{X,Y} = \\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{var}[X] \\operatorname{var}[Y]}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Cauchy&ndash;Schwarz inequality (**§1.2.6**),\n",
    "\n",
    "$$\\vert \\operatorname{Cov}(X,Y) \\vert \\leq \\sqrt{\\operatorname{var}[X]\\operatorname{var}[Y]},$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$-1 \\leq \\rho(X,Y) \\leq 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te equality in the Cauchy-Schwarz inequality is achieved if two variables are linearly dependent, and so $\\vert \\rho \\vert = 1$ if\n",
    "\n",
    "$$Y = aX + b$$\n",
    "\n",
    "for some constants $a$ and $b$. Note, in addition, that $a > 0$ implies $\\rho = 1$, and that $a < 0$ implies $\\rho < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.13. Random variables on product spaces.** Let $(\\Omega_1,\\mathcal{F}_1,\\mathbb{P}_1)$ and $(\\Omega_2,\\mathcal{F}_2,\\mathbb{P}_2)$ be probability spaces. We recall that the **product probability space** $(\\Omega_1 \\times \\Omega_2, \\mathbb{F}_1 \\otimes \\mathbb{F}_2, \\mathbb{P}_1 \\otimes \\mathbb{P}_2)$ consists of the cartesian product $\\Omega_1 \\times \\Omega_2$ of event spaces, the $\\sigma$-algebra $\\mathcal{F}_1 \\otimes \\mathcal{F}_2$ generated by rectangles on $(\\mathcal{F}_1,\\mathcal{F}_2)$, and multiplicatively defined product probability measure $\\mathcal{P}_1 \\otimes \\mathcal{P}_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given fixed $\\omega_1 \\in \\Omega_1$ and $\\omega_2 \\in \\Omega_2$, the **canonical injection mappings** $\\iota^1_{\\omega_2}:\\Omega_1 \\to \\Omega_1 \\times \\Omega_2$ and $\\iota^2_{\\omega_1}:\\Omega_2 \\to \\Omega_1 \\times \\Omega_2$ defined by the formulas\n",
    "\n",
    "$$\\iota^1_{\\omega_2}(\\omega) = (\\omega, \\omega_2) \\hspace{1em}\\mbox{and}\\hspace{1em} \\iota^2_{\\omega_1}(\\omega) = (\\omega_1, \\omega)$$\n",
    "\n",
    "are $(\\mathcal{F}_i, \\mathcal{F}_1 \\otimes \\mathcal{F}_2)$-measurable for $i = 1, 2$, respectively. This implies that the mappings \n",
    "\n",
    "$$X_1(\\omega) = X(\\omega, \\omega_2) \\hspace{1em}\\mbox{and}\\hspace{1em} X_2(\\omega) = X(\\omega_1, \\omega)$$\n",
    "\n",
    "are random variables whenever $X:\\Omega_1 \\times \\Omega_2 \\to \\mathbb{R}$ is $\\mathcal{F}_1 \\otimes \\mathcal{F}_2$-measurable. Indeed,\n",
    "\n",
    "$$X_1 = X \\circ \\iota^1_{\\omega_2} \\hspace{1em} \\mbox{and} \\hspace{1em} X_2 = X \\circ \\iota^2_{\\omega_1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fubini's theorem** states that\n",
    "\n",
    "$$\\int_{\\Omega_1 \\times \\Omega_2} X(\\omega_1,\\omega_2) \\, d(\\omega_1,\\omega_2) = \\int_{\\Omega_1} \\int_{\\Omega_2} X(\\omega_1,\\omega_2) \\, d\\omega_2 \\, d\\omega_1 = \\int_{\\Omega_2} \\int_{\\Omega_1} X(\\omega_1,\\omega_2) \\, d\\omega_1 \\, d\\omega_2$$\n",
    "\n",
    "whenever\n",
    "\n",
    "$$\\int_{\\Omega_1 \\times \\Omega_2} \\left\\vert X(\\omega_1,\\omega_2) \\right\\vert \\, d(\\omega_1,\\omega_2) < \\infty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tonelli's theorem** states that the above identity holds whenever $X$ is a nonnegative $(\\mathcal{F}_1 \\otimes \\mathcal{F}_2)$-measurable random variable on $\\Omega_1 \\times \\Omega_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often refer to both theorems simultaneously as the **Fubini&ndash;Tonelli theorem**. We note that the Fubini&ndash;Tonelli theorem holds on every measure space $(\\Omega,\\mathcal{F},\\mu)$ that is **$\\sigma$-finite**, i.e., there exists a sequence $(E_n)_{n=1}^\\infty$ of $\\mu$-finite sets in $\\mathcal{F}$ such that\n",
    "\n",
    "$$\\bigcup_{n=1}^\\infty E_n = \\Omega.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple application, we consider the product measure space $(\\Omega \\times \\mathbb{R}, \\mathcal{F} \\otimes \\mathscr{B}_{\\mathbb{R}}, \\mathbb{P} \\otimes \\mathscr{L})$ constructed form a probability space $(\\Omega,\\mathcal{F}, \\mathbb{P})$ and the Lebesgue measure space $(\\mathbb{R},\\mathscr{B}_{\\mathbb{R}},\\mathscr{L})$. We fix a random variable $X:\\Omega \\to \\mathbb{R}$ and define\n",
    "\n",
    "$$A_X = \\{(\\omega, t) : 0 \\leq t \\leq X(\\omega)\\}.$$\n",
    "\n",
    "Observe that the indicator function $\\boldsymbol{1}_{A_X}:\\Omega \\times \\mathbb{R} \\to \\mathbb{R}$ satisfies the following identities:\n",
    "\n",
    "$$\\mathbb{E}[\\boldsymbol{1}_{A_X} \\circ \\iota^1_t] = \\mathbb{P}[X \\geq t] \\hspace{1em}\\mbox{and}\\hspace{1em} \\mathbb{E}[\\boldsymbol{1}_{A_X} \\circ \\iota^2_{\\omega}] = \\mathscr{L}([0,X(\\omega)]) = X(\\omega).$$\n",
    "\n",
    "It follows from the Fubini&ndash;Tonelli theorem that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\boldsymbol{1}_{A_X}] &= \\int_{\\Omega} \\int_{-\\infty}^\\infty \\boldsymbol{1}_{A_X}(\\omega, t) \\, dt \\, d\\omega = \\mathbb{E}[X] \\\\\n",
    "\\mathbb{E}[\\boldsymbol{1}_{A_X}] &= \\int_{-\\infty}^\\infty \\int_{\\Omega} \\boldsymbol{1}_{A_X}(\\omega, t) \\, d\\omega \\, dt = \\int_0^\\infty \\mathbb{P}[X \\geq t] \\, dt,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "whence\n",
    "\n",
    "$$\\mathbb{E}[X] = \\int_0^\\infty \\mathbb{P}[X \\geq t] \\, dt.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1. Binomial and Bernoulli distributions.** Given the usual discrete probability space $(\\{H,T\\}^n, \\mathcal{P}(\\{H,T\\}^n), \\mathbb{P})$ modeling of tossing a fair coin $n$ times (**§1.1.4**), we define the random variable $X$ on it by the formula\n",
    "\n",
    "$$X(\\omega_1,\\ldots,\\omega_n) = \\sum_{i=1}^n \\boldsymbol{1}_{H}(\\omega_i),$$\n",
    "\n",
    "where $\\boldsymbol{1}_H(x_i)$ is 1 if $\\omega_i = 1$ and 0 otherwise. In this case, the probability mass function $p_X(k)$ yields the number of possible coin-tossing outcomes with $k$ many heads, divided by $2^n$. It is not hard to see that\n",
    "\n",
    "$$2^n(h+t)^n = \\sum_{j=0}^n p_X(j)h^jt^{n-j}$$\n",
    "\n",
    "where $p_X(a) = \\mathbb{P}[X=a]$. It follows that\n",
    "\n",
    "$$p_X(k) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} 2^{-n} = \\frac{n!}{(n-k)!k!} 2^{-n}$$\n",
    "\n",
    "whenever $k \\in \\{0,1,\\ldots,n-1,n\\}$.\n",
    "\n",
    "More generally, if the probability of heads is $\\theta$, then the above polynomial formulation yields\n",
    "\n",
    "$$p_X(k) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} \\theta^k (1-\\theta)^{n-k}$$\n",
    "\n",
    "whenever $k \\in \\{0,1,\\ldots,n-1,n\\}.$ In this case, we say that $X$ has a **binomial distribution**, and write $\\operatorname{Bin}(k \\mid n, \\theta)$ to denote $p_X(k)$. If $n= 1$, then we say that $X$ has a **Bernoulli distribution** and write $\\operatorname{Ber}(x \\mid \\theta)$ to denote $p_X(x)$.\n",
    "\n",
    "We remark that an experiment consisting of repeatedly performing *independent* tasks with only two outcomes is called a **Bernoulli trial**. Our coin-tossing example is the archetypal example of a Bernoulli trial.  Many other scenarios can be modeled as Bernoulli trials, so long as *success* and *failure* can be clearly defined.\n",
    "\n",
    "To compute its moments, we consider its moment-generating function $M_X(t) = \\mathbb{E}[e^{tX}]$ (**§1.2.10**). By the change-of-variables formula (**§1.2.7**), we have the identity\n",
    "\n",
    "$$\\begin{align*} \\mathbb{E}[e^{tX}] &= \\sum_{k=0}^n e^{tk} \\operatorname{Bin}(k \\mid n, \\theta) \\\\ &= \\sum_{k=0}^n e^{tk} \\begin{pmatrix} n \\\\\\ k \\end{pmatrix} \\theta^k (1-\\theta)^{n-k} \\\\ &= \\sum_{k=0}^n \\begin{pmatrix} n \\\\\\ k \\end{pmatrix}  (\\theta e^t)^k (1-\\theta)^{n-k} \\\\ &= (\\theta e^t + (1-\\theta))^n. \\end{align*}$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$M_X'(t) = n(\\theta e^t + (1-\\theta))^{n-1}(\\theta e^t),$$\n",
    "\n",
    "we have $\\mu = M_X'(0) = n\\theta.$ Moreover,\n",
    "\n",
    "$$\\begin{align*}M_X''(t) =& n(n-1)(\\theta e^t + (1-\\theta))^{n-2}(\\theta e^t)^2 \\\\ &+ n(\\theta e^t + (1-\\theta))^{n-1}(\\theta e^t),\\end{align*}$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\mu_2 = M_X''(0) = n(n-1) \\theta^2 + n\\theta = n^2 \\theta^2 + n\\theta(1- \\theta).$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\\operatorname{var}(X) = n^2 \\theta^2 + n\\theta(1-\\theta) - n^2\\theta^2 = n\\theta(1-\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2. Poisson approximation of the binomial distribution.** Suppose that we have performed a large number of Bernoulli trials with small $\\theta$, so that the mean of the binomial distribution $$\\mu = n\\theta$$ is of \"moderate magnitude\". In such cases, we can derive a convenient approximation to the distribution, due to Poisson.\n",
    "\n",
    "Recall that $\\operatorname{Bin}(0 \\mid n, \\theta) = (1-\\theta)^n = (1-\\frac{\\mu}{n})^n$.  Since\n",
    "\n",
    "$$\\log (1 + x) = \\sum_{j=1}^\\infty \\frac{(-1)^{j+1}}{j} x^j = \\frac{x}{j} + o(x^2),$$\n",
    "\n",
    "we see that\n",
    "\n",
    "$$\\begin{align*} \\log \\left(\\operatorname{Bin}(0 \\mid n, \\theta)\\right) &= n \\log \\left( 1 - \\frac{\\mu}{n}\\right) \\\\ &= n \\sum_{j=1}^\\infty - \\frac{(\\mu/n)^j}{j} \\\\ &= n\\left( \\frac{-\\mu}{n} + o(n^{-2}) \\right) \\\\ &= -\\mu + o(n^{-1}). \\end{align*}$$\n",
    "\n",
    "Therefore, $\\operatorname(0 \\mid n, \\theta) \\approx e^{-\\mu}$. \n",
    "\n",
    "Now,\n",
    "\n",
    "$$\\frac{\\operatorname{Bin}(k \\mid n, \\theta)}{\\operatorname{Bin}(k-1 \\mid n, \\theta)} = \\frac{(n-k+1)\\theta}{k(1-\\theta)} \\approx \\frac{n \\theta}{k} = \\frac{\\mu}{k},$$\n",
    "\n",
    "as we have assumed that $n$ is large and $\\theta$ is small. Therefore,\n",
    "\n",
    "$$\\operatorname{Bin}(k \\mid n, \\theta) \\approx \\frac{\\mu}{k} \\operatorname{Bin}(k-1 \\mid n, \\theta) \\approx \\cdots \\approx \\frac{\\mu^k}{k!} e^{-\\mu}.$$\n",
    "\n",
    "What scenarios might have large $n$ and small $\\theta$? We could consider, for example, the probability of $k$ people having their birthdays on New Year's Day ($\\theta = 1/365$), given that the group of people is large. We could also  consider the probability of $k$ factory-produced items being defective, given a low fraction defective. In other words, the Poisson approximation is appropriate for modeling *rare events*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.3. Poisson distribution.** In light of the Poisson approximation of the Binomial distribution, we say that a natural-number-valued random variable $X$ has a **Poisson distribution** with parameter $\\lambda > 0$ if its probability mass function equals\n",
    "\n",
    "$$p_X(k) = \\operatorname{Poi}(k \\mid \\lambda) = e^{-\\lambda} \\frac{\\lambda^k}{k!}.$$\n",
    "\n",
    "Let us consider an example from [Rutherford, Chadwick, and Ellis, *Radiations From Radioactive Substances*](https://books.google.com/books?id=hOx-ZDIztiQC&lpg=PA171&vq=172&pg=PA172#v=snippet&q=172&f=false). In a radioactive decay experiment, emissions of $\\alpha$-particles were observed 2608 times. The total count of $\\alpha$-particles observed was 10097, so that the average number of new particles appearing during a unit time interval (7.5 seconds) was about 3.87. Using the Poisson distribution with $\\lambda = 3.87$, we expect the following:\n",
    "\n",
    "| $k$ | $2608$ $\\times$ $\\operatorname{Poi}$ $(k$ $\\mid$ $3.87$ $)$ |\n",
    "| ----- | ---------------------------------------- |\n",
    "| 0     | 54                                       |\n",
    "| 1     | 210                                      |\n",
    "| 2     | 407                                      |\n",
    "| 3     | 525                                      |\n",
    "| 4     | 508                                      |\n",
    "| 5     | 394                                      |\n",
    "| 6     | 254                                      |\n",
    "| 7     | 140                                      |\n",
    "| 8     | 68                                       |\n",
    "| 9     | 29                                       |\n",
    "| 10    | 11                                       |\n",
    "| 11    | 4                                        |\n",
    "| 12    | 1                                        |\n",
    "| 13    | 1                                        |\n",
    "| 14    | 1                                        |\n",
    "\n",
    "Here $\\operatorname{Poi}(k \\mid 3.87)$ represents the expected probability that $k$ new particles are observed in a unit time interval. Therefore, $ 2608 \\times \\operatorname{Poi}(k \\mid 3.87)$ yields the expected number of the unit time intervals in which $k$ new particles are observed.\n",
    "\n",
    "Compare the above with the actual number of occurrence:\n",
    "\n",
    "| Number of particles observed in interval | Observed number of occurrences |\n",
    "| ---------------------------------------- | ------------------------------ |\n",
    "| 0                                        | 57                             |\n",
    "| 1                                        | 203                            |\n",
    "| 2                                        | 383                            |\n",
    "| 3                                        | 525                            |\n",
    "| 4                                        | 532                            |\n",
    "| 5                                        | 408                            |\n",
    "| 6                                        | 273                            |\n",
    "| 7                                        | 139                            |\n",
    "| 8                                        | 45                             |\n",
    "| 9                                        | 27                             |\n",
    "| 10                                       | 10                             |\n",
    "| 11                                       | 4                              |\n",
    "| 12                                       | 0                              |\n",
    "| 13                                       | 1                              |\n",
    "| 14                                       | 1                              |\n",
    "\n",
    "Other scenarios that conform to the Poisson distribution include chromosome interchange in cells exposed to X-ray irradiation, telephone connections to wrong numbers, bacterial colonies distribution in a Petri dish, and so on.\n",
    "\n",
    "Using the change-of-variables formula, we can compute the moment-generating function $M_X(t) = \\mathbb{E}[e^{tX}]$ of the Poisson distribution as follows: \n",
    "\n",
    "$$\\begin{align*} M_X(t) &= \\sum_{k=0}^\\infty e^{tk} \\operatorname{Poi}(k \\mid \\lambda) \\\\ &= e^{-\\lambda} \\sum_{k=0}^\\infty  \\frac{(\\lambda e^t)^k}{k!} \\\\ &= e^{\\lambda(e^t - 1)}. \\end{align*}$$\n",
    "\n",
    "Since $M_X'(t) = e^{\\lambda(e^t - 1)}(\\lambda e^t)$, we see that the mean is $\\mu = M_X'(0) = \\lambda$. Moreover,\n",
    "\n",
    "$$M_X''(t) = e^{\\lambda (e^t - 1)} (\\lambda e^t)^2 + e^{\\lambda (e^t - 1)} (\\lambda e^t),$$\n",
    "\n",
    "and so $\\mu_2 = M_X''(0) = \\lambda^2 + \\lambda$. Therefore,\n",
    "\n",
    "$$\\operatorname{var}[X] = \\mu_2 - \\mu^2 = \\lambda.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.4. Normal approximation to the binomial distribution.** We once again consider a scenario in which we perform a large number of Bernoulli trials&mdash;this time, with $\\theta = 1/2$. We let $n = 2\\nu$ and define\n",
    "\n",
    "$$a_k = \\operatorname{Bin}\\left(\\nu + k \\mid 2\\nu, \\frac{1}{2} \\right)$$\n",
    "\n",
    "for each $-\\nu \\leq k \\leq \\nu$, so that the terms of the binomial distribution $\\operatorname{Bin}(\\cdot \\mid n, \\frac{1}{2} )$ are represented by the sequence\n",
    "\n",
    "$$a_{-\\nu}, a_{-\\nu+1}, \\cdots, a_{-1},a_0,a_1,\\cdots,a_{\\nu-1},a_{\\nu}.$$\n",
    "\n",
    "For $k \\geq 0$, we have that\n",
    "\n",
    "$$\\begin{align*}\n",
    "a_k\n",
    "&= \\operatorname{Bin} \\left( \\nu + k \\mid 2\\nu, \\frac{1}{2} \\right) \\\\\n",
    "&= \\frac{(2\\nu)!}{(\\nu - k)! (\\nu + k)!} 2^{-2\\nu} \\\\\n",
    "&= \\frac{\\nu ! \\nu !}{(\\nu - k)! (\\nu + k)!} \\frac{(2\\nu)!}{\\nu ! \\nu !} 2^{-2\\nu} \\\\\n",
    "&= \\frac{\\nu \\cdot (\\nu - 1) \\cdots (\\nu -k + 1)}{(\\nu + 1) \\cdot (\\nu + 2) \\cdots (\\nu + k)} a_0 \\\\\n",
    "&= \\frac{1 \\cdot (1 - \\frac{1}{\\nu}) \\cdots (1 - \\frac{k-1}{\\nu})}{(1 + \\frac{1}{\\nu}) (1 + \\frac{2}{\\nu}) \\cdots (1 + \\frac{k}{\\nu})} a_0.\n",
    "\\end{align*}$$\n",
    "\n",
    "We, of course, have $a_k = a_{-k}$, and so the above formula characterizes all terms of $\\operatorname{Bin}(\\cdot \\mid n, \\frac{1}{2})$ in terms of $a_0$. Now, <a href=\"https://en.wikipedia.org/wiki/Taylor's_theorem\">Taylor's theorem</a> implies that\n",
    "\n",
    "$$1 + \\frac{j}{\\nu} = e^{j/\\nu} + o(\\nu^{-1}),$$\n",
    "\n",
    "and so, for large enough $\\nu$,\n",
    "\n",
    "$$a_k \\approx \\frac{e^{0/\\nu} \\cdot e^{-1/\\nu} \\cdots e^{-(k-1)/\\nu}}{e^{0/\\nu} \\cdot e^{1/\\nu} \\cdots e^{k/\\nu}} a_0 \\approx e^{-k^2/\\nu} a_0.$$\n",
    "\n",
    "It follows from <a href=\"https://en.wikipedia.org/wiki/Stirling's_approximation\">Stirling's formula</a> that\n",
    "\n",
    "$$a_k \\approx e^{-k^2/\\nu} a_0 = e^{-k^2/\\nu} \\begin{pmatrix} 2\\nu \\\\ \\nu \\end{pmatrix} 2^{-2\\nu} \\approx e^{-k^2/\\nu} \\cdot \\frac{1}{\\sqrt{\\pi \\nu}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.5. Gaussian integrals.** In light of the normal approximation of the binomial distribution, we consider random variables with a **Gaussian function**\n",
    "\n",
    "$$a e^{-b(x-c)^2}$$\n",
    "\n",
    "as their probability density functions (Section [2.8](#2-8)). It might appear initially that Gaussian distributions have *three* parameters&mdash;$a$, $b$, and $c$&mdash;but one of them, $a$, depends on the other two. Given $b$ and $c$, $a$ must be chosen to ensure that the resulting function integrates to 1.\n",
    "\n",
    "We begin our study of Gaussian functions by computing the integral of $e^{-x^2}$. Note first that\n",
    "\n",
    "$$\\left( \\int_{-\\infty}^\\infty e^{-x^2} \\, dx \\right)^2 = \\int_{-\\infty}^\\infty e^{-x^2} \\, dx \\int_{-\\infty}^\\infty e^{-y^2} \\, dy = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-(x^2+y^2)} \\, dx \\, dy.$$\n",
    "\n",
    "Now, we take the [polar coordinate transform](https://en.wikipedia.org/wiki/Multiple_integral#Polar_coordinates), which yields $r^2=  x^2 + y^2$, and observe that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-(x^2+y^2)} \\, dx \\, dy\n",
    "&= \\int_0^\\infty \\int_0^{2\\pi} e^{-r^2}r \\, d\\theta \\, dr \\\\\n",
    "&= 2\\pi \\int_0^\\infty e^{-r^2}r \\, dr \\\\\n",
    "&= 2\\pi \\int_{-\\infty}^0 \\frac{1}{2} e^s \\, ds \\\\\n",
    "&= \\pi.\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, an additional coordinate transform $-r^2 \\mapsto s$ has been made. It follows that\n",
    "\n",
    "$$ \\int_{-\\infty}^\\infty e^{-x^2} \\, dx = \\sqrt{\\pi},$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty a e^{-b(x-c)^2} \\, dx = \\frac{a}{\\sqrt{b}} \\int_{-\\infty}^\\infty e^{-t^2} \\, dt = a \\sqrt{\\frac{\\pi}{b}}.$$\n",
    "\n",
    "From this, we see that any Gaussian probability density function must be of the form\n",
    "\n",
    "$$\\sqrt{\\frac{b}{\\pi}} e^{-b(x-c)^2}.$$\n",
    "\n",
    "Now, we let $X$ be a random variable with a Gaussian probability density function, so that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mu &= \\int_{-\\infty}^\\infty x \\sqrt{ \\frac{b}{\\pi}} e^{-b(x-c)^2} \\, dx \\\\\n",
    "\\mu_2 &= \\int_{-\\infty}^\\infty x^2 \\sqrt{ \\frac{b}{\\pi}} e^{-b(x-c)^2} \\, dx.\n",
    "\\end{align*}$$\n",
    "\n",
    "We evaluate the first integral by taking the coordinate transform $\\sqrt{b}(x-c) = u$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mu &= \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\left( \\frac{1}{\\sqrt{b}} u + c \\right) e^{-u^2} \\, du \\\\\n",
    "&= \\frac{1}{\\sqrt{b \\pi}} \\int_{-\\infty}^\\infty u e^{-u^2} \\, du\n",
    "+ \\frac{c}{\\sqrt{\\pi}} e^{-u^2} \\, du \\\\\n",
    "  &= \\frac{1}{\\sqrt{b \\pi}} \\int_{-\\infty}^\\infty u e^{-u^2} \\, du + c.\n",
    "  \\end{align*}$$\n",
    "\n",
    "Another coordinate transform, $u^2 = t$, yields\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\int_{-\\infty}^\\infty u e^{-u^2} \\, du\n",
    "&= \\frac{1}{2} \\inf_{u = -\\infty}^{u = \\infty} e^{-t} \\, dt \\\\\n",
    "&= -\\frac{e^{-t}}{2} \\bigg|_{u = -\\infty}^{u = \\infty} \\\\\n",
    "&= -\\frac{e^{-u^2}}{2} \\bigg|_{u=-\\infty}^{u = \\infty} = 0,\n",
    "\\end{align*}$$\n",
    "\n",
    "and so we conclude that $\\mu = c$.\n",
    "\n",
    "As for the variance, we begin, once again, by  take the coordinate transform $\\sqrt{b}(x-c) = u$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mu_2\n",
    "&= \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\left(\\frac{1}{\\sqrt{b}} u + c\\right)^2 e^{-u^2} \\, du \\\\\n",
    "&= \\frac{1}{\\sqrt{\\pi}}\n",
    "\\left(\n",
    "\\frac{1}{b}  \\int_{-\\infty}^\\infty u^2 e^{-u^2} \\, du\n",
    "+ \\frac{2c}{\\sqrt{b}} \\int_{-\\infty}^\\infty u e^{-u^2} \\, du\n",
    "+ c^2 \\int_{-\\infty}^\\infty e^{-u^2} \\, du \\right) \\\\\n",
    "  &= \\frac{1}{\\sqrt{\\pi}} \\left( \\frac{1}{b}  \\int_{-\\infty}^\\infty u^2 e^{-u^2} \\, du + 0 + c^2 \\sqrt{\\pi} \\right) \\\\\n",
    "  &= \\frac{1}{b\\sqrt{\\pi}} \\int_{-\\infty}^\\infty u^2  e^{-u^2} \\, du + c^2.\n",
    "  \\end{align*}$$\n",
    "\n",
    "To evaluate\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty u^2 e^{-u^2} \\, du,$$\n",
    "\n",
    "we note that\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty e^{-vu^2} \\, du = \\sqrt{\\frac{\\pi}{v}}$$\n",
    "\n",
    "and [differentiate the integral](https://en.wikipedia.org/wiki/Leibniz_integral_rule) with respect to $v$:\n",
    "\n",
    "$$-\\frac{\\sqrt{\\pi}}{2} v^{-3/2} = \\frac{d}{dv} \\sqrt{\\frac{pi}{v}} = \\int_{-\\infty}^\\infty \\frac{d}{dv} e^{-vu^2} \\, du\n",
    "= \\int_{-\\infty}^\\infty -u^2e^{-vu^2} \\, du.$$\n",
    "\n",
    "We now let $v = 1$ to conclude that\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty u^2 e^{-u^2} \\, du = \\frac{\\sqrt{\\pi}}{2},$$\n",
    "\n",
    "whence it follows that\n",
    "\n",
    "$$\\mu_2 = \\frac{1}{2b} + c^2 .$$\n",
    "\n",
    "We compute the variance as follows:\n",
    "\n",
    "$$\\sigma^2 = \\mu_2 - \\mu^2 = \\frac{1}{2b} + c^2 - c^2 = \\frac{1}{2b}.$$\n",
    "\n",
    "\n",
    "All in all, we have shown that $a = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}$,  $b = \\frac{1}{2\\sigma^2}$, and $c = \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.6. Normal distribution.** We discussed how to approximate the binomial distribution using Gaussian functions $a e^{-b(x-c)^2}$. We have shown that a random variable with $a e^{-b(x-c)^2}$ as its probability density function has mean $\\mu = c$ and variance $\\sigma^2 = \\frac{1}{2b}$. Since $a$ must equal $\\sqrt{\\frac{b}{\\pi}}$ for the integral of the Gaussian probability density function to be 1, it makes sense to write\n",
    "\n",
    "$$\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}$$\n",
    "\n",
    "to represent the pdf of the normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "The cumulative distribution function of the normal distribution is then\n",
    "\n",
    "$$\\Phi(x; \\mu, \\sigma^2) = \\int_{-\\infty}^x \\mathcal{N}(t \\mid \\mu, \\sigma^2) \\, dt.$$\n",
    "\n",
    "While the above integral has no closed-form expression, the **error function**\n",
    "\n",
    "$$\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} \\, dt$$\n",
    "\n",
    "allows us to write the cumulative distribution function as follows:\n",
    "\n",
    "$$\\Phi(x; \\mu, \\sigma^2) = \\frac{1}{2} \\left( 1 + \\operatorname{erf}(x) \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.1. Introduction.** Given a probability space $(\\Omega,\\mathcal{F}, \\mathbb{P})$ and two events $A, B \\in \\mathcal{F}$, what should the **probability of $A$ given $B$** be? To answer this question, we can create a new sample space where $B$ always happens, i.e., all the events intersect with $B$. Formally, we take $\\Omega_B = \\Omega \\cap B$ and\n",
    "\n",
    "$$\\mathcal{F}_B = \\{E \\cap B : E \\in \\mathcal{F}\\}.$$\n",
    "\n",
    "The construction implies that every $E \\in \\mathcal{F}_B$ admits $E' \\in \\mathcal{F}$ such that $E = E' \\cap B$. It thus makes sense to define the new probability measure $\\mathbb{P}_B[E]$ in terms of $\\mathbb{P}[E' \\cap B]$. Note, however, that $\\mathbb{P}$ restricted to $\\mathcal{F}_B$ is not necessarily a probability measure, as its maximum value is $\\mathbb{P}[B]$. We therefore take the normalization\n",
    "\n",
    "$$\\mathbb{P}_B[E] = \\frac{\\mathbb{P}[E' \\cap B]}{\\mathbb{P}[B]},$$\n",
    "\n",
    "so that $\\mathbb{P}_B$ is a *bona fide* probability measure.\n",
    "\n",
    "With this construction, it would be reasonable to say that the probability of $A$ given $B$ is $\\mathbb{P}_B[A]$. We thus define\n",
    "\n",
    "$$\\mathbb{P}[A \\mid B] = \\mathbb{P}_B[A] = \\frac{\\mathbb{P}[A \\cap B]}{\\mathbb{P}[B]},$$\n",
    "\n",
    "provided that $\\mathbb{P}[B] > 0$. If $\\mathbb{P}[B] = 0$, conditioning is meaningless, as $\\mathbb{P}[A \\mid B]$ would have to be $0$ for any event $A$. \n",
    "\n",
    "Given the above notion of conditional probability, we declare two events $A$ and $B$ to be **independent** if\n",
    "\n",
    "$$\\mathbb{P}[A \\mid B] = \\mathbb{P}[A] \\hspace{1em} \\mbox{and} \\hspace{1em} \\mathbb{P}[B \\mid A] = \\mathbb{P}[B].$$\n",
    "\n",
    "This holds if and only if $\\mathbb{P}[A \\cap B] = \\mathbb{P}[A]\\mathbb{P}[B]$, which is often taken as the definition of independence.\n",
    "\n",
    "As an example of independence, consider the experiment of tossing an unbiased coin twice. The event $A$ that the first toss turns up heads is, intuitively, independent of the event $B$ that the second toss turns up heads. Since $A = \\{$ `HH`, `HT`$\\}$, $B = \\{$ `HH`, `TH` $\\}$, and $A \\cap B = \\{$ `HH` $\\}$, we see that\n",
    "\n",
    "$$\\mathbb{P}[A \\cap B] = \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2} = \\mathbb{P}[A] \\mathbb{P}[B].$$\n",
    "\n",
    "Therefore, our intuition coincides with the definition in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.2. Independence.** Recall that two events $A$ and $B$ from a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ are **independent** if $\\mathbb{P}[A \\cap B] = \\mathbb{P}[A] \\mathbb{P}[B]$. It is not hard to extend this definition to several events by declaring $E_1,\\ldots,E_n$ to be independent if\n",
    "\n",
    "$$\\mathbb{P}\\left[ \\bigcap_{j=1}^n E_j \\right] = \\prod_{j=1}^n \\mathbb{P}[E_j].$$\n",
    "\n",
    "Extending the definition further, we say that $\\sigma$-algebras $\\mathcal{G}_1,\\ldots,\\mathcal{G}_n$ such that $\\mathcal{G}_j \\subseteq \\mathcal{F}$ for all $1 \\leq j \\leq n$ are **independent $\\sigma$-subalgebras of $\\mathcal{F}$** in case\n",
    "\n",
    "$$\\mathbb{P} \\left[ \\bigcap_{j=1}^n G_j \\right] = \\prod_{j=1}^n \\mathbb{P}[G_j]$$\n",
    "\n",
    "whenever $G_j \\in \\mathcal{G}_j$ for each $1 \\leq j \\leq n$.\n",
    "\n",
    "How do we make sense of the notion of independent $\\sigma$-algebras? Suppose we wish to say that two collections of events $\\mathscr{E} = \\{E_1,\\ldots,E_n\\}$ and $\\mathscr{F} = \\{F_1,\\ldots,F_m\\}$ are independent. We should then expect any probabilistic information we can deduce from $\\mathscr{E}$ to not depend on $\\mathscr{F}$, and vice versa. The natural way to guarantee this is to stipulate that any event that can be generated from $\\mathscr{E}$ be independent of any event generated from $\\mathscr{F}$. In other words, $\\sigma(\\mathscr{E})$ and $\\sigma(\\mathscr{F})$ must be independent.\n",
    "\n",
    "We push further the idea of $\\sigma$-algebras as the total collection of available information and define the **$\\sigma$-algebra generated by a random variable $X$** to be the $\\sigma$-algebra generated by the set of all preimages of $X$, i.e.,\n",
    "\n",
    "$$\\sigma(X) = \\sigma \\left( \\left\\{ \\{\\omega: X((\\omega) \\in B\\} : B \\in \\mathscr{B}_{\\mathbb{R}} \\right\\} \\right),$$\n",
    "\n",
    "where $\\mathscr{B}_{\\mathbb{R}}$ is the Borel $\\sigma$-algebra on $\\mathbb{R}$.\n",
    "\n",
    "With this definition, we say that random variables $X_1,\\ldots,X_n$ are **independent random variables** if the corresponding $\\sigma$-algebras are independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.3. Conditional expectation.** Given a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, we consider the Hilbert space $L_2(\\Omega,\\mathcal{F},\\mathbb{P})$ of random variables $X:\\Omega \\to \\mathbb{R}$ such that $\\mathbb{E}[\\vert X \\vert^2] < \\infty$. We wish to compute the expectation of $X$ when we do not have all the information in $\\mathcal{F}$ available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To formalize, we let $\\mathcal{G}$ be a $\\sigma$-subalgebra of $\\mathcal{F}$. Since a $\\mathcal{G}$-measurable function on $\\Omega$ is always $\\mathcal{F}$-measurable, we see that the Hilbert space $L_2(\\Omega,\\mathcal{G},\\mathbb{P})$ (**§1.2.6**) is a Hilbert subspace of $L_2(\\Omega,\\mathcal{F},\\mathbb{P})$. The limited information availability is modeled by considering the least-squares estimation $Y \\in L_2(\\Omega,\\mathcal{G},\\mathbb{P})$, i.e., $\\mathcal{G}$-measurable random variable $Y$ that minimizes\n",
    "\n",
    "$$\\|Y-X\\|_{L_2(\\Omega,\\mathcal{F},\\mathbb{P})}.$$\n",
    "\n",
    "A standard result from Hilbert space theory tells us that such an estimation is computed by taking the orthogonal projection $P:L_2(\\Omega,\\mathcal{F},\\mathbb{P}) \\to L_2(\\Omega,\\mathcal{G},\\mathbb{P})$, under which $PX = Y$. The linear operator $P$ is called the **conditional expectation** given $\\mathcal{G}$ and is denoted by\n",
    "\n",
    "$$PX = \\mathbb{E}[X \\mid \\mathcal{G}].$$\n",
    "\n",
    "If $\\mathcal{G} = \\sigma(Z)$ for some random variable $Z$, then we typically write\n",
    "\n",
    "$$\\mathbb{E}[X \\mid Z]$$\n",
    "\n",
    "to denote $\\mathbb{E}[X \\mid \\sigma(Z)]$. The conditional expectation constructed this way satisfies the monotone convergence theorem and the dominated convergence theorem (**§1.2.5**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\mathbb{E}[\\cdot\\mid \\mathcal{G}]$ is constructed to be a linear operator, we have that\n",
    "\n",
    "$$\\mathbb{E}[aX + bY\\mid \\mathcal{G}] = a \\mathbb{E}[X\\mid \\mathcal{G}] + b \\mathbb{E}[Y \\mid \\mathcal{G}].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover $\\mathbb{E}[X \\mid \\mathcal{G}] \\geq 0$ whenever $X \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional expectation satisfies a form of **Jensen's inequality**: if $c:\\mathbb{R} \\to \\mathbb{R}$ is [convex](https://en.wikipedia.org/wiki/Convex_function), then\n",
    "\n",
    "$$c(\\mathbb{E}[X \\mid \\mathcal{G}]) \\leq \\mathbb{E}[c(X) \\mid \\mathcal{G}].$$\n",
    "\n",
    "In particular, $t \\mapsto t^p$ is convex for all $p \\geq 1$, and so \n",
    "\n",
    "$$\\mathbb{E}[\\vert X \\vert \\mid \\mathcal{G}]^p \\leq \\mathbb{E}[ \\vert X \\vert^p \\mid \\mathcal{G}].$$\n",
    "\n",
    "Moreover $t \\mapsto \\vert\\cdot\\vert$ is convex, and so\n",
    "\n",
    "$$\\mathbb{E}[\\vert X \\vert \\mid \\mathcal{G}]^p \\geq \\left \\vert \\mathbb{E}[X \\mid \\mathcal{G}] \\right\\vert^p.$$\n",
    "\n",
    "Takking the expectation, we see that\n",
    "\n",
    "$$\\|\\mathbb{E}[X \\mid \\mathcal{G}]\\|_p \\leq \\mathbb{E}[\\mathbb{E}[\\vert X \\vert^p \\mid \\mathcal{G}]]^{1/p}= \\|X\\|_p.$$\n",
    "\n",
    "The last equality follows from the **law of total expectation**:\n",
    "\n",
    "$$\\mathbb{E}[\\mathbb{E}[Y \\mid \\mathcal{G}]] = \\mathbb{E}[Y].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathcal{H}$ is a $\\sigma$-subalgebra of $\\mathcal{G}$, then the **tower property**\n",
    "\n",
    "$$\\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{G}] \\mid \\mathcal{H}] = \\mathbb{E}[X \\mid \\mathcal{H}]$$\n",
    "\n",
    "holds. This is obtain by projecting $X$ to $L_2(\\Omega,\\mathcal{G},\\mathbb{P})$, and then to $L_2(\\Omega,\\mathcal{H},\\mathbb{P})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Hölder's inequality (**§1.2.6**), \n",
    "\n",
    "$$\\mathbb{E}[XY \\mid \\mathcal{G}] = X \\mathbb{E}[Y \\mid \\mathcal{G}]$$\n",
    "\n",
    "whenever $X \\in L_p(\\Omega,\\mathcal{G},\\mathbb{P})$ and $Y \\in L_{p'}(\\Omega,\\mathcal{G},\\mathbb{P})$ with $\\frac{1}{p} + \\frac{1}{p'} = 1$. In particular, if $Y$ is a $\\mathcal{F}$-measurable random variable and $X$ is a bounded $\\mathcal{G}$-measurable random variable, then\n",
    "\n",
    "$$\\mathbb{E}[XY] = X\\mathbb{E}[Y].$$\n",
    "\n",
    "Since we interpret $\\mathbb{E}[\\cdot \\mid \\mathcal{G}]$ in the context of *knowing only the information provided by $\\mathcal{G}$*, we can think of the above identity as *taking out what is known*, as per <a href=\"https://books.google.com/books?id=e9saZ0YSi-AC&lpg=PP1&vq=%22taking%20out%20what's%20known%22&pg=PA89#v=onepage&q&f=false\">David Williams</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if $X$ is independent of $\\mathcal{G}$, then\n",
    "\n",
    "$$\\mathbb{E}[X \\mid \\mathcal{G}] = \\mathbb{E}[X],$$\n",
    "\n",
    "as knowing $\\mathcal{G}$ has no bearing on the expected value of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.4. Conditional probabilities.** A general form of conditional probabilities can be derived as a special case of the conditional expectation. Given a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, $\\sigma$-subspace $\\mathcal{G}$ of $\\mathcal{F}$, and an element $A$ of $\\mathcal{F}$, we define\n",
    "\n",
    "$$\\mathbb{P}[A \\mid \\mathcal{G}] = \\mathbb{E}[\\boldsymbol{1}_A \\mid \\mathcal{G}].$$\n",
    "\n",
    "Observe that, for each $G \\in \\mathcal{G}$,\n",
    "\n",
    "$$\\int_G \\mathbb{P}[A \\mid \\mathcal{G}] \\, d\\mathbb{P} = \\mathbb{E}[\\boldsymbol{1}_G\\mathbb{E}[\\boldsymbol{1}_A \\mid \\mathcal{G}]] = \\mathbb{E}[\\mathbb{E}[\\boldsymbol{1}_G\\boldsymbol{1}_A \\mid \\mathcal{G}]]$$\n",
    "\n",
    "because $\\boldsymbol{1}_G$ is bounded and $\\mathcal{G}$-measurable. The law of total expectation now implies that\n",
    "\n",
    "$$\\int_G \\mathbb{P}[A \\mid \\mathcal{G}] \\, d\\mathbb{P} = \\mathbb{E}[\\boldsymbol{1}_G \\boldsymbol{1}_A] = \\mathbb{P}[G \\cap A].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that $\\mathbb{P}[A \\mid \\mathcal{G}] = \\mathbb{P}[A]$ almost surely (**§1.1.3**) if and only if $\\boldsymbol{1}_A$ is independent of $\\mathcal{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a conditional probability is the conditional expectation of a nonnegative function,\n",
    "\n",
    "$$\\mathbb{P}[A \\mid \\mathcal{G}] \\geq 0.$$\n",
    "\n",
    "Moreover,\n",
    "\n",
    "$$\\mathbb{P}[A \\mid \\mathcal{G}] \\leq \\mathbb{P}[\\Omega \\mid \\mathcal{G}] = \\mathbb{E}[\\boldsymbol{1}_\\Omega] = 1$$\n",
    "\n",
    "because  $\\boldsymbol{1}_\\Omega$ is independent of $\\mathcal{G}$. We note, in particular, that $\\mathbb{P}[\\varnothing \\mid \\mathcal{G}] = 0$ and $\\mathbb{P}[\\Omega \\mid \\mathcal{G}] = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the monotone convergence theorem (**§1.2.5**, **§1.4.3**) and the linearity of the conditional expectation,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sum_{n=1}^\\infty \\mathbb{P}[A_n \\mid \\mathcal{G}]\n",
    "&= \\sum_{n=1}^\\infty \\mathbb{E}[\\boldsymbol{1}_{A_n} \\mid \\mathcal{G}] \\\\\n",
    "&= \\lim_{N \\to \\infty} \\sum_{n=1}^N \\mathbb{E} [\\boldsymbol{1}_{A_n} \\mid \\mathcal{G}] \\\\\n",
    "&= \\lim_{N \\to \\infty} \\mathbb{E} \\left[\\sum_{n=1}^N \\boldsymbol{1}_{A_n} \\mid \\mathcal{G}\\right] \\\\\n",
    "&= \\lim_{N \\to \\infty} \\mathbb{E} \\left[\\boldsymbol{1}_{\\bigcup_{n=1}^N A_n} \\mid \\mathcal{G}\\right] \\\\\n",
    "&= \\mathbb{E} \\left[\\lim_{N \\to \\infty} \\boldsymbol{1}_{\\bigcup_{n=1}^N A_n} \\mid \\mathcal{G}\\right] \\\\\n",
    "&= \\mathbb{E} \\left[ \\boldsymbol{1}_{\\bigcup_{n=1}^\\infty A_n} \\mid \\mathcal{G} \\right] \\\\\n",
    "&= \\mathbb{P} \\left[ \\bigcup_{n=1}^N A_n \\mid \\mathcal{G} \\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "whenever $(A_n)_{n=1}^\\infty$ is a disjoint collection of events. It follows that $\\mathbb{P}[\\cdot \\mid \\mathcal{G}]$ can be considered a probability measure on $(\\Omega,\\mathcal{F})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.5. Conditional distributions.** Recall that there is a one-to-one correspondence between probability distributions and the Lebesgue&ndash;Stieltjes measures (**§1.2.9**). In light of this, we shall construct conditional distributions as probability measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space, $\\mathcal{G}$ a $\\sigma$-subalgebra of $\\mathcal{F}$, and $X$ a $\\mathcal{F}$-measurable random variable. The function $\\mu:\\mathscr{B}_\\mathbb{R} \\times \\Omega \\to [0,1]$ such that\n",
    "\n",
    "- $\\mu(\\cdot,\\omega)$ is a probability measure on $(\\Omega,\\mathcal{F})$ for each $w \\in \\Omega$, and that\n",
    "- $\\mu(E,\\cdot) = \\mathbb{P}[X \\in E \\mid \\mathcal{G}]$ almost surely for each $E \\in \\mathscr{B}_{\\mathbb{R}}$\n",
    "\n",
    "is called a **conditional distribution of $X$ given $\\mathcal{G}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also recall that, given a random variable $Y$ on $(\\Omega,\\mathcal{F})$ and a Borel measurable function $\\varphi:\\mathbb{R} \\to \\mathbb{R}$, we have the **change-of-variables formula**\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty \\varphi(\\alpha) \\, dF_Y(\\alpha)$$\n",
    "\n",
    "(**§1.2.7**, **§1.2.8**, **§1.2.9**). Similarly, conditional distributions are linked to the conditional expectation via the conditional version of the change-of-variables fomrula:\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty \\varphi(\\alpha) \\, d\\mu(\\alpha,\\omega) = \\mathbb{E}[\\varphi(X) \\mid \\mathcal{G}](\\omega).$$\n",
    "\n",
    "In particular, if $\\varphi(\\alpha) = \\alpha$, then\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty \\alpha \\, d\\mu(\\alpha,\\omega) = \\mathbb{E}[X \\mid \\mathcal{G}](\\omega).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Multivariate Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5.1. Random vectors.** Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space. A **random $n$-vector** is a function $X:\\Omega \\to \\mathbb{R}^n$ whose coordinate functions $X_1,\\ldots,X_n$ are random variables on $(\\Omega,\\mathcal{F},\\mathbb{P})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\mathscr{B}_{\\mathbb{R}}^{\\otimes n}$ agrees with $\\mathscr{B}_{\\mathbb{R}^n}$, the above definition is equivalent to $X:\\Omega \\to \\mathbb{R}^n$ being $(\\mathcal{F},\\mathscr{B}_{\\mathbb{R}^n})$-measurable, i.e., $E \\in \\mathscr{B}_{\\mathbb{R}^n}$ implies $X^{-1}(E) \\in \\mathcal{F}$. This, in particular, implies that Borel measurable functions preserve random vectors. Indeed, if $g:\\mathbb{R}^n \\to \\mathbb{R}^m$ is $(\\mathscr{B}_{\\mathbb{R}^n},\\mathscr{B}_{\\mathbb{R}^m})$-measurable, then $g(X):\\Omega \\to \\mathbb{R}^m$ is a random $m$-vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the sum function $(x_1,\\ldots,x_n) \\mapsto x_1 + \\cdots + x_n$ is Borel measurable, and so it follows that the sum of random variables is a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of a random vector is defined componentwise:\n",
    "\n",
    "$$\\mathbb{E}[X] = (\\mathbb{E}X_1,\\ldots,\\mathbb{E}X_n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5.2. Variance-Covariance.** The **variance-covariance matrix** of a random vector $X = (X_1,\\ldots,X_n)$ is the $n$-by-$n$ matrix $\\Sigma$ whose entires are given by the formula\n",
    "\n",
    "$$\\Sigma_{ij} = \\operatorname{Cov}(X_i, X_j).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the variance-covariance matrix to define the **correlation matrix**:\n",
    "\n",
    "$$\\rho(X)_{ij} = \\rho(X_i,X_j).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that the variance-covariance matrix is symmetric and positive-semidefinite. (Conversely, every symmetric and positive-semidefinite matrix is the variance-covariance matrix of a random vector.) By the spectral theorem, $\\Sigma_{ij}$ can be unitarily diagonalized with nonnegative entries on the diagonal, a fact useful in [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). If all the diagonal entries are positive, then the inverse $\\Sigma^{-1}$ exists, called the **precision matrix**. The [square root](https://en.wikipedia.org/wiki/Square_root_of_a_matrix) of $\\Sigma^{-1}$ is commonly used for [whitening transforms](https://en.wikipedia.org/wiki/Whitening_transformation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5.3. Joint probability distributions.** Given a random vector $X = (X_1,\\ldots,X_n)$, we define the **distribution** of $X$ to be the mapping $F_X:\\mathbb{R}^n \\to [0,1]$, given by the formula\n",
    "\n",
    "$$F_X(t_1,\\ldots,t_n) = \\mathbb{P}[X_1 \\leq t_1; \\cdots ; X_n \\leq t_n].$$\n",
    "\n",
    "$F_X$ is also called the **joint probability distribution** of $X_1,\\ldots,X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$f_X:\\mathbb{R}^n \\to \\mathbb{R}$ is the **joint probability density function** of $X_1,\\ldots,X_n$ in case\n",
    "\n",
    "$$\\mathbb{P}[X \\in E] =  \\int_E f_X \\, d\\mathscr{L}_{\\mathbb{R}^n}$$\n",
    "\n",
    "for all $E \\in \\mathscr{B}_{\\mathbb{R}^n}$, where $\\mathscr{L}_{\\mathbb{R}^n} = \\mathscr{L}_{\\mathbb{R}} \\otimes \\cdots \\otimes \\mathscr{L}_{\\mathbb{R}}$ the Lebesgue measure on $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**1.5.4. Multivariate Gaussian.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of a random vector is defined componentwise:\n",
    "\n",
    "$$\\mathbb{E}[X] = (\\mathbb{E}X_1,\\ldots,\\mathbb{E}X_n).$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
